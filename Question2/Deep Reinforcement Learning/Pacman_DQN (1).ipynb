{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pacman-DQN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1P-fPXP_LTC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5d06887d-5b1a-4a9c-dca4-cf382787b9e5"
      },
      "source": [
        "import gym\n",
        "import sys\n",
        "import pylab\n",
        "import random\n",
        "import os\n",
        "import operator\n",
        "from collections import deque\n",
        "\n",
        "from skimage import io, color, transform\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Model,Sequential\n",
        "from keras.layers import Flatten,Conv2D,Input,Dense,MaxPooling2D,Activation\n",
        "import keras.backend as K\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "GAME_TYPE = ''\n",
        "env =gym.make(\"MsPacman-v0\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN1IfBlG_MAq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "92bb464f-34c5-4daa-8501-27333ff7fc7a"
      },
      "source": [
        "#Hyperparameters\n",
        "\n",
        "#Environment parameters\n",
        "NUM_EPISODES=80000000\n",
        "#We feed the model 4 frames at a time\n",
        "PHI_LENGTH=4\n",
        "\n",
        "#Agent parameters\n",
        "EPSILON=1\n",
        "EXPERIENCE_REPLAY_CAPACITY=2000\n",
        "MINIBATCH_SIZE=100\n",
        "LEARNING_RATE=0.01\n",
        "ACTION_SIZE=env.action_space.n\n",
        "EXPLORE=3000000\n",
        "UPDATE_RATE=10000\n",
        "\n",
        "PREPROCESS_IMAGE_DIM=84 #We downsize the atari frame to 84 x 84\n",
        "STATE_SIZE=(PREPROCESS_IMAGE_DIM,PREPROCESS_IMAGE_DIM,4)\n",
        "#STATE_SIZE=(88,80,1)\n",
        "#Adding a parameter loss function to experiment with different losses\n",
        "print(ACTION_SIZE)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWmVjd4s_NVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "    #\n",
        "    #Initialization\n",
        "    #\n",
        "    def __init__(self, state_size,epsilon , experience_replay_capacity , minibatch_size , learning_rate ,action_size, img_dim,explore):\n",
        "        #self.loss_func=loss_func\n",
        "        self.state_size=state_size\n",
        "        self.action_size=action_size\n",
        "        self.discount_factor=0.90\n",
        "        self.learning_rate=learning_rate\n",
        "        self.epsilon=epsilon\n",
        "        self.epsilon_min=0.05\n",
        "        self.batch_size=minibatch_size\n",
        "        self.train_start=1000\n",
        "        self.explore=explore\n",
        "        self.img_channels=4 #phi_length  #coz we feed in 4 stacked b&w imgs instead of 1 rbg img\n",
        "        self.processed_image_dim=img_dim\n",
        "        \n",
        "         # create replay memory using deque\n",
        "        self.D=deque(maxlen=experience_replay_capacity)\n",
        "        # create main model and target model\n",
        "        self.model=self.build_model()\n",
        "        self.target_model=self.build_model()\n",
        "        #self.target_model.set_weights(self.model.get_weights())\n",
        "        # initialize target model\n",
        "        self.update_target_model()\n",
        "        #self.model.summary()\n",
        "    \n",
        "    def build_model(self):\n",
        "        model=Sequential()\n",
        "        \n",
        "        model.add(Conv2D(32,(8,8),strides=4,padding=\"same\",input_shape=self.state_size))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        \n",
        "        model.add(Conv2D(64,(4,4),strides=2,padding=\"same\"))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        \n",
        "        model.add(Conv2D(64,(3,3),strides=1,padding=\"same\"))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Flatten())\n",
        "        \n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        \n",
        "        model.compile(loss='mse', optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=None, decay=0.0))\n",
        "        \n",
        "        print(\"finish building the model\")\n",
        "        print(model.summary())\n",
        "        \n",
        "        return model\n",
        "    \n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "        \n",
        "    def append_experience_replay_example(self,s_t,a_t,r_t,s_t1,done):\n",
        "        \"\"\"\n",
        "        Add an experience replay example to our agent's replay memory. If\n",
        "        memory is full, overwrite previous examples, starting with the oldest\n",
        "        \"\"\"\n",
        "        #D is a memory cell\n",
        "        #Records State,Action,Reward,Next State and the boolean done\n",
        "        self.D.append((s_t, a_t, r_t, s_t1, done))\n",
        "    \n",
        "    def preprocess_observation(self, observation, prediction=False):\n",
        "        \"\"\"\n",
        "        Helper function for preprocessing an observation for consumption by our\n",
        "        deep learning network\n",
        "        \"\"\"\n",
        "        grayscale_observation = color.rgb2gray(observation)\n",
        "        resized_observation = transform.resize(grayscale_observation, (1,self.processed_image_dim, self.processed_image_dim)).astype('float32')\n",
        "        if prediction:\n",
        "            resized_observation = np.expand_dims(resized_observation,0)\n",
        "        return resized_observation\n",
        "    \n",
        "    #This idea is copied from https://github.com/ageron/tiny-dqn/blob/master/tiny_dqn.py\n",
        "    #def preprocess_observation(self,frame):\n",
        "        #mspacman_color = np.array([210, 164, 74]).mean()\n",
        "        #img = frame[1:176:2, ::2]    # Crop and downsize\n",
        "        #img = img.mean(axis=2)       # Convert to greyscale\n",
        "        #img[img==mspacman_color] = 0 # Improve contrast by making pacman white\n",
        "        #img = (img - 128) / 128 - 1  # Normalize from -1 to 1.\n",
        "        #return np.expand_dims(img.reshape(88, 80, 1), axis=0)\n",
        "    \n",
        "    def take_action(self, s_t):\n",
        "        \"\"\"\n",
        "        Given an observation, the model attempts to take an action\n",
        "        according to its q-function approximation\n",
        "        \"\"\"\n",
        "        #We take an action based on our current epsilon value\n",
        "        #This is called Epsilon greedy exploration/exploitation\n",
        "        \n",
        "        if np.random.rand()<=self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        actions=self.model.predict(s_t)\n",
        "        return np.argmax(actions[0])\n",
        "    \n",
        "    def learn(self, batch_size):\n",
        "        \n",
        "        minibatch = random.sample(self.D, batch_size)\n",
        "        \n",
        "        for s_t, a_t, r_t, s_t1, done in minibatch:\n",
        "            if not done:\n",
        "                max_action = np.argmax(self.model.predict(s_t1)[0])\n",
        "                target = (r_t + self.discount_factor * self.target_model.predict(s_t1)[0][max_action])\n",
        "            else:\n",
        "                target = r_t\n",
        "            #The idea of target vector taken from a medium post:\n",
        "            # 1. Use the current model to output the Q-value predictions\n",
        "            target_f = self.model.predict(s_t)\n",
        "            # 2. Rewrite the chosen action value with the computed target\n",
        "            target_f[0][a_t]=target\n",
        "            # 3. Use vectors in the objective computation\n",
        "            self.model.fit(s_t, target_f, epochs=1, verbose=0)\n",
        "    \n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon -= (self.epsilon - self.epsilon_min) /self.explore\n",
        "    \n",
        "    def train_minibatch(self,batch_size):\n",
        "    \n",
        "      \"\"\"Prepare X_batch, y_batch and train them\n",
        "      Recall our loss function is\n",
        "          target = reward + discount * max Q(s',a)\n",
        "                   or reward if done early\n",
        "          Loss function: [target - Q(s, a)]^2\n",
        "      Hence,\n",
        "          X_batch is a state list\n",
        "          y_batch is reward + discount * max Q\n",
        "                     or reward if terminated early\n",
        "      Args:\n",
        "          DQN (dqn.DQN): DQN Agent to train & run\n",
        "          train_batch (list): Minibatch of Replay memory\n",
        "              Eeach element is a tuple of (s, a, r, s', done)\n",
        "      Returns:\n",
        "          loss: Returns a loss\n",
        "      \"\"\"\n",
        "      train_batch = random.sample(self.D, batch_size)\n",
        "      state_array = np.vstack([x[0] for x in train_batch])\n",
        "      action_array = np.array([x[1] for x in train_batch])\n",
        "      reward_array = np.array([x[2] for x in train_batch])\n",
        "      next_state_array = np.vstack([x[3] for x in train_batch])\n",
        "      done_array = np.array([x[4] for x in train_batch])\n",
        "      print(state_array.shape)\n",
        "      X_batch = state_array\n",
        "      y_batch = self.model.predict(state_array)\n",
        "\n",
        "      Q_target = reward_array + self.discount_factor * np.max(self.model.predict(next_state_array), axis=1)* ~done_array\n",
        "      y_batch[np.arange(len(X_batch)), action_array] = Q_target\n",
        "\n",
        "      # Train our network using target and predicted Q values on each episode\n",
        "      self.model.fit(X_batch, y_batch)\n",
        "\n",
        "           "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGO72UIT_QHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_simulation():\n",
        "    \"\"\"\n",
        "    Entry-point for running env simulation\n",
        "    \"\"\"\n",
        "\n",
        "    #print game parameters\n",
        "    print (\"~~~Environment Parameters~~~\")\n",
        "    print (\"Num episodes: %s\" % NUM_EPISODES)\n",
        "    print (\"Action space: %s\" % env.action_space)\n",
        "    print()\n",
        "    print (\"~~~Agent Parameters~~~\")\n",
        "    print (\"Epsilon: %s\" % EPSILON)\n",
        "    print (\"Experience Replay Capacity: %s\" % EXPERIENCE_REPLAY_CAPACITY)\n",
        "    print (\"Minibatch Size: %s\" % MINIBATCH_SIZE)\n",
        "    print (\"Learning Rate: %s\" % LEARNING_RATE)\n",
        "\n",
        "    #initialize agent\n",
        "    agent = Agent(state_size=STATE_SIZE,epsilon=EPSILON,\n",
        "                experience_replay_capacity=EXPERIENCE_REPLAY_CAPACITY,\n",
        "                minibatch_size=MINIBATCH_SIZE,\n",
        "                learning_rate=LEARNING_RATE, action_size =ACTION_SIZE, img_dim =PREPROCESS_IMAGE_DIM ,explore =EXPLORE)\n",
        "    \n",
        "    scores, episodes = [], [] \n",
        "\n",
        "    #initialize auxiliary data structures\n",
        "    state_list = [] \n",
        "    #tot_frames = 0\n",
        "\n",
        "    for i_episode in range(NUM_EPISODES):\n",
        "        print (\"Episode: %s\" % i_episode)\n",
        "        tot_frames=0\n",
        "        done = False\n",
        "        score = 0\n",
        "        x_t=env.reset()\n",
        "        x_t=agent.preprocess_observation(x_t)   \n",
        "        s_t=np.stack((x_t, x_t, x_t, x_t), axis=3) \n",
        "        #how many consecutive frames to stack depends on your PHI\n",
        "        \n",
        "        while not done:\n",
        "          #env.render()\n",
        "          # get action for the current state and go one step in environment\n",
        "          a_t=agent.take_action(s_t)\n",
        "          x_t1,r_t,done,_=env.step(a_t)\n",
        "          # get action, change score and learn from memory\n",
        "          score+=r_t\n",
        "          \n",
        "          x_t1=agent.preprocess_observation(x_t1)   \n",
        "          x_t1 = x_t1.reshape(x_t1.shape[0], x_t1.shape[1], x_t1.shape[2],1)\n",
        "          #x_t1 = np.reshape(x_t1, (84, 84, 1))\n",
        "\n",
        "          s_t1 = np.append(x_t1, s_t[ : , :, :, :3], axis=3)\n",
        "          agent.append_experience_replay_example(s_t,a_t,r_t,s_t1,done)\n",
        "\n",
        "          #FILL THIS\n",
        "          s_t=s_t1\n",
        "\n",
        "        if done:\n",
        "          # every episode update the target model to be same with model\n",
        "          agent.update_target_model() \n",
        "          scores.append(score)\n",
        "          episodes.append(i_episode)\n",
        "          \n",
        "\n",
        "          print( \"  score:\", score, \"  epsilon:\", agent.epsilon)\n",
        "          \n",
        "        while True:\n",
        "          #ensure state list is populated\n",
        "          if tot_frames < PHI_LENGTH:\n",
        "            state_list.append(x_t)\n",
        "            tot_frames+=1\n",
        "            #print(tot_frames)\n",
        "            #print(state_list)\n",
        "\n",
        "            continue\n",
        "            \n",
        "          else:\n",
        "            #update state list with next observation\n",
        "            state_list.append(x_t)\n",
        "            state_list.pop(0)\n",
        "\n",
        "            break\n",
        "        \n",
        "        agent.learn(MINIBATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw6Djd83_VXO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4f817110-b381-485f-db2f-c04fb1b4e091"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  env =gym.make(\"MsPacman-v0\")    \n",
        "  run_simulation()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "~~~Environment Parameters~~~\n",
            "Num episodes: 100\n",
            "Action space: Discrete(9)\n",
            "\n",
            "~~~Agent Parameters~~~\n",
            "Epsilon: 1\n",
            "Experience Replay Capacity: 2000\n",
            "Minibatch Size: 100\n",
            "Learning Rate: 0.01\n",
            "finish building the model\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_67 (Conv2D)           (None, 21, 21, 32)        8224      \n",
            "_________________________________________________________________\n",
            "activation_67 (Activation)   (None, 21, 21, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_68 (Conv2D)           (None, 11, 11, 64)        32832     \n",
            "_________________________________________________________________\n",
            "activation_68 (Activation)   (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_69 (Conv2D)           (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_69 (Activation)   (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_23 (Flatten)         (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_89 (Dense)             (None, 128)               991360    \n",
            "_________________________________________________________________\n",
            "dense_90 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_91 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_92 (Dense)             (None, 9)                 585       \n",
            "=================================================================\n",
            "Total params: 1,094,697\n",
            "Trainable params: 1,094,697\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "finish building the model\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_70 (Conv2D)           (None, 21, 21, 32)        8224      \n",
            "_________________________________________________________________\n",
            "activation_70 (Activation)   (None, 21, 21, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_71 (Conv2D)           (None, 11, 11, 64)        32832     \n",
            "_________________________________________________________________\n",
            "activation_71 (Activation)   (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_72 (Conv2D)           (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_72 (Activation)   (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_24 (Flatten)         (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_93 (Dense)             (None, 128)               991360    \n",
            "_________________________________________________________________\n",
            "dense_94 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_95 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_96 (Dense)             (None, 9)                 585       \n",
            "=================================================================\n",
            "Total params: 1,094,697\n",
            "Trainable params: 1,094,697\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Episode: 0\n",
            "  score: 160.0   epsilon: 1\n",
            "Episode: 1\n",
            "  score: 270.0   epsilon: 0.9999996833333333\n",
            "Episode: 2\n",
            "  score: 300.0   epsilon: 0.9999993666667722\n",
            "Episode: 3\n",
            "  score: 180.0   epsilon: 0.9999990500003167\n",
            "Episode: 4\n",
            "  score: 170.0   epsilon: 0.9999987333339667\n",
            "Episode: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-58cff6695431>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MsPacman-v0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mrun_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-d7934661d89a>\u001b[0m in \u001b[0;36mrun_simulation\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m           \u001b[0mscore\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mr_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m           \u001b[0mx_t1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m           \u001b[0mx_t1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_t1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_t1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_t1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m           \u001b[0;31m#x_t1 = np.reshape(x_t1, (84, 84, 1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-e96aa4b9acf5>\u001b[0m in \u001b[0;36mpreprocess_observation\u001b[0;34m(self, observation, prediction)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \"\"\"\n\u001b[1;32m     71\u001b[0m         \u001b[0mgrayscale_observation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgb2gray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mresized_observation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrayscale_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_image_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_image_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mresized_observation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresized_observation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         image = ndi.gaussian_filter(image, anti_aliasing_sigma,\n\u001b[0;32m--> 149\u001b[0;31m                                     cval=cval, mode=ndi_mode)\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;31m# 2-dimensional interpolation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/ndimage/filters.py\u001b[0m in \u001b[0;36mgaussian_filter\u001b[0;34m(input, sigma, order, output, mode, cval, truncate)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             gaussian_filter1d(input, sigma, axis, order, output,\n\u001b[0;32m--> 299\u001b[0;31m                               mode, cval, truncate)\n\u001b[0m\u001b[1;32m    300\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/ndimage/filters.py\u001b[0m in \u001b[0;36mgaussian_filter1d\u001b[0;34m(input, sigma, axis, order, output, mode, cval, truncate)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;31m# Since we are calling correlate, not convolve, revert the kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_gaussian_kernel1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorrelate1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/ndimage/filters.py\u001b[0m in \u001b[0;36mcorrelate1d\u001b[0;34m(input, weights, axis, output, mode, cval, origin)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ni_support\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_mode_to_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     _nd_image.correlate1d(input, weights, axis, output, mode, cval,\n\u001b[0;32m---> 95\u001b[0;31m                           origin)\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr1nQ-gVBTdw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}