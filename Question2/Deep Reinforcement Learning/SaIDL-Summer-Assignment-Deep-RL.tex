\documentclass{article}

\title{SAiDL-2019-Summer Assignment-2019(Markov Decision Process)}
\author{Ayush Aaryan}
\usepackage{amsmath}


\usepackage{amssymb}
\begin{document}


\maketitle
\pagenumbering{gobble}
%\newpage
\pagenumbering{arabic}

\section{Question}


You receive the following letter:\newline
Dear Friend,
Some time ago, I bought this old house, but found it to be haunted by
ghostly sardonic laughter. As a result it is hardly habitable. There is hope,
however, for by actual testing I have found that this haunting is subject to
certain laws, obscure but infallible, and that the laughter can be affected by
my playing the organ or burning incense. In each minute, the laughter occurs
or not, it shows no degree. What it will do during the ensuing minute depends,
in the following exact way, on what has been happening during the preceding
minute: Whenever there is laughter, it will continue in
the succeeding minute unless I play the organ, in which case it will stop. But
continuing to play the organ does not keep the house quiet. I notice, however,
that whenever I burn incense when the house is quiet and do not play the
organ it remains quiet for the next minute. At this minute of writing, the
laughter is going on. Please tell me what manipulations of incense and organ
I should make to get that house quiet, and to keep it so.\newline
Sincerely,\newline
At Wits End

\subsection{Formulate this problem as an MDP (for the sake of uniformity , formulate it as
a continuing discounted problem with $\gamma=0.9$. Let the reward be +1 on any
transition into the silent state, and -1 on any transition into the laughing state).
Explicitly give the state set, action sets, state transition and reward function.}


\subsection{Starting with the policy $\pi(laughing)=\pi(silent)=\pi(incense, no organ)$,
perform a couple of policy iterations(by hand) until you find an optimal policy(
Clearly show and label each step. If you are taking a lot of iterations, stop and
reconsider your formulation). Do a couple of value iterations as well.
}


\subsection{What are the resulting optimal state-action values for all state-action pairs?}

\subsection{What is your advice to At Wits End?}


\newpage
\pagenumbering{arabic}

\section{Answers}

\subsection{a.State Set=\{L,S\},where L refers to Laughter and S refers to Silent.\newline
	     b.Action Set=$\{ \neg I \land O , \neg I \land \neg O,  I \land O, I \land \neg O\}$,where I refers to burning incense sticks and O refers to playing organ.\newline
             c.State Transition=\{$P_{L \to L}$,$P_{L \to S}$,$P_{S \to L}$, $P_{S \to S}$\}\newline
d.}
\subsection{Finding an Optimal Policy}
\subsubsection{Policy Iteration}
Step1:Arbritary Assignment of Policy: $\pi (L)=\pi(S)=I\land\neg O$\newline\newline
Step2: $\gamma=0.9$\newline\newline
$V(L)=P_{L\to L}^{\pi(L)}(R_{L\to L}^{\pi(L)}+\gamma V(L))+P_{L\to S}^{\pi(L)}(R_{L\to S}^{\pi(L)}+\gamma V(S))=-1$\newline\newline
$V(S)=P_{S\to L}^{\pi(L)}(R_{S\to L}^{\pi(S)}+\gamma V(L))+P_{S\to S}^{\pi(S)}(R_{S\to S}^{\pi(S)}+\gamma V(S))=1$\newline\newline
Step3: $\gamma=0.9$\newline\newline
$V(L)=P_{L\to L}^{\pi(L)}(R_{L\to L}^{\pi(L)}+\gamma V(L))+P_{L\to S}^{\pi(L)}(R_{L\to S}^{\pi(L)}+\gamma V(S))=-1.9$\newline\newline
$V(S)=P_{S\to L}^{\pi(L)}(R_{S\to L}^{\pi(S)}+\gamma V(L))+P_{S\to S}^{\pi(S)}(R_{S\to S}^{\pi(S)}+\gamma V(S))=1.9$\newline\newline
Step4: $\gamma=0.9$\newline\newline
$V(L)=P_{L\to L}^{\pi(L)}(R_{L\to L}^{\pi(L)}+\gamma V(L))+P_{L\to S}^{\pi(L)}(R_{L\to S}^{\pi(L)}+\gamma V(S))=-2.71$\newline\newline
$V(S)=P_{S\to L}^{\pi(L)}(R_{S\to L}^{\pi(S)}+\gamma V(L))+P_{S\to S}^{\pi(S)}(R_{S\to S}^{\pi(S)}+\gamma V(S))=+2.71$\newline\newline
Hence,we see that V(S) is improving whereas V(L) is decreasing.So,let's try a new policy-:\newline\newline$\pi(L)=O \land I \newline\newline \pi(S)=\neg O \land I $\newline\newline
Step5: $\gamma=0.9$\newline\newline
$V(L)=P_{L\to L}^{\pi(L)}(R_{L\to L}^{\pi(L)}+\gamma V(L))+P_{L\to S}^{\pi(L)}(R_{L\to S}^{\pi(L)}+\gamma V(S))=+3.44$\newline\newline
$V(S)=P_{S\to L}^{\pi(L)}(R_{S\to L}^{\pi(S)}+\gamma V(L))+P_{S\to S}^{\pi(S)}(R_{S\to S}^{\pi(S)}+\gamma V(S))=+3.44$\newline\newline
This looks like a good policy but lets perform another iteration to be sure that this is the optimal policy.\newline\newline
Step5: $\gamma=0.9$\newline\newline
$V(L)=P_{L\to L}^{\pi(L)}(R_{L\to L}^{\pi(L)}+\gamma V(L))+P_{L\to S}^{\pi(L)}(R_{L\to S}^{\pi(L)}+\gamma V(S))=+4.10$\newline\newline
$V(S)=P_{S\to L}^{\pi(L)}(R_{S\to L}^{\pi(S)}+\gamma V(L))+P_{S\to S}^{\pi(S)}(R_{S\to S}^{\pi(S)}+\gamma V(S))=+4.10$\newline\newline
Hence,there is still improvement.Hence,this is the optimal policy.
\subsubsection{Value Iteration}
In value iteration,we first set the values to 0 and then find the values for different actions, and choose the one which gives the maximum q reward.\newline\newline
Step1: V(L)=V(S)=0\newline\newline
Step2:\newline\newline $V(L)=P_{L\to L}^{action}(R_{L\to L}^{action}+\gamma V(L))+P_{L\to S}^{action}(R_{L\to S}^{action}+\gamma V(S))$\newline\newline
We find that V(L) is maximum for action=\{$O \land I,O \land \neg I$\} \newline\newline
$V(S)=P_{S\to L}^{action}(R_{S\to L}^{action}+\gamma V(L))+P_{S\to S}^{action}(R_{S\to S}^{action}+\gamma V(S))$\newline\newline
We find that V(L) is maximum for action=\{$\neg O \land I$\} \newline\newline
Step3: Iterating again following the same set of actions.\newline\newline
V(L)=+1.9\newline\newline
V(S)=+1.9\newline\newline
Step4: Iterating again following the same set of actions.\newline\newline
V(L)=+2.71\newline\newline
V(S)=+2.71\newline\newline
So,we see that the improvement over each iteration is decreasing/converging to an optimum to which we set our values,this convergence follows a GP with common ratio $\gamma$=0.9.So,now the final value can be found out after discounting rewards.\newline

\subsection{State-Action Table}

Whenever we take the optimal action first and continue taking optimal actions,we get the full reward(+10) and when we take a random action and then take optimal actions we get 0.8 of the reward.$(-1(for the first action)+\gamma*total-reward).$\newline\newline
$L \Longrightarrow Laughter State$\newline
$S \Longrightarrow Silent State$\newline
$O \Longrightarrow Playing Organ$\newline
$I \Longrightarrow Burning Incense Stick$\newline

	\begin{table}[h!]
		\begin{tabular}{c|c|c|c}
			CurrentState & Action & NewState & Optimal State-Action values\\
			\hline\newline
			S & $O \land I$ & L & +8\\ 
			S & $O \land \neg I$ & L & +8\\
			S & $\neg O \land I $ & S & +10 \\
			S & $\neg O \land \neg I$ & L & +8 \\
			L & $O \land I$ & S & +10\\ 
			L & $O \land \neg I$ & S & +10\\
			L & $\neg O \land \neg I$ & L & +8\\
			L & $\neg O \land  I$ & L & +8\\
		\end{tabular}
	\end{table}


\subsection{My advice to "At Wits End" would be if the room is silent, do not play the organ and burn incense and if there is laughter,play the organ}

\end{document}