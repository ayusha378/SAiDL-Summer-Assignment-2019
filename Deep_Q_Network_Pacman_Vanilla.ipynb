{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GFpxggVF3Iiw"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import sys\n",
    "import pylab\n",
    "import random\n",
    "import os\n",
    "import operator\n",
    "from collections import deque\n",
    "\n",
    "from skimage import io, color, transform\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import Flatten,Convolution2D,Input,Dense\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "GAME_TYPE = ''\n",
    "env =gym.make(\"MsPacman-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "okS4eYuK3LSj",
    "outputId": "66b43025-82d1-4675-8410-7d4e237bff2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameters\n",
    "\n",
    "#Environment parameters\n",
    "NUM_EPISODES=100\n",
    "#We feed the model 4 frames at a time\n",
    "PHI_LENGTH=4\n",
    "\n",
    "#Agent parameters\n",
    "EPSILON=1\n",
    "EXPERIENCE_REPLAY_CAPACITY=2000\n",
    "MINIBATCH_SIZE=100\n",
    "LEARNING_RATE=0.01\n",
    "ACTION_SIZE=env.action_space.n\n",
    "EXPLORE=3000000\n",
    "PREPROCESS_IMAGE_DIM=84 #We downsize the atari frame to 84 x 84\n",
    "\n",
    "#Adding a parameter loss function to experiment with different losses\n",
    "ERROR_FUNCTION=tf.losses.huber_loss\n",
    "print(ACTION_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "OIQUUXsd31F_",
    "outputId": "cba16b8d-2af1-4e5e-b7a1-7429b54f6c35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State.shape = (210, 160, 3)\n",
      "State_size = 100800\n",
      "State.shape now = (1, 100800)\n",
      "State_size now = 100800\n",
      "env.observation_space.shape = (210, 160, 3)\n",
      "next_State.shape = (210, 160, 3)\n",
      "next_State.shape now = (1, 100800)\n",
      "x_t.shape = (84, 84)\n",
      "s_t.shape = (84, 84, 4)\n",
      "s_t.shape now = (1, 84, 84, 4)\n",
      "(210, 160, 3)\n",
      "x_t1.shape = (84, 84)\n",
      "x_t1.shape now = (1, 84, 84, 1)\n",
      "s_t1.shape = (1, 84, 84, 4)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Use this cell just to explore the environment, not used towards the algo\n",
    "'''\n",
    "state =env.reset()\n",
    "#print state\n",
    "print(\"State.shape = %s\" % (state.shape,))\n",
    "print (\"State_size = %s\" % (state.size,))\n",
    "#laying out all features in 1 row, for fun\n",
    "state = np.reshape(state, [1, state.size])\n",
    "#print state\n",
    "print (\"State.shape now = %s\" % (state.shape,))\n",
    "print (\"State_size now = %s\" % (state.size,))\n",
    "#print env.observation_space\n",
    "print (\"env.observation_space.shape = %s\" %(env.observation_space.shape,))\n",
    "\n",
    "action = random.randrange(env.action_space.n)\n",
    "next_state, reward, done, info = env.step(action)\n",
    "#print next_state\n",
    "print (\"next_State.shape = %s\" % (next_state.shape,))\n",
    "next_state = np.reshape(state, [1, next_state.size])\n",
    "#print next_state\n",
    "print (\"next_State.shape now = %s\" % (next_state.shape,))\n",
    "def preprocess_observation(observation,prediction=False):\n",
    "    \"\"\"\n",
    "    Helper function for preprocessing an observation for consumption by our\n",
    "    deep learning network\n",
    "    \"\"\"\n",
    "    grayscale_observation = color.rgb2gray(observation)\n",
    "    processed_image_dim=PREPROCESS_IMAGE_DIM\n",
    "    resized_observation = transform.resize(grayscale_observation, (processed_image_dim,processed_image_dim)).astype('float32')\n",
    "    if prediction:\n",
    "        resized_observation = np.expand_dims(resized_observation, 0)\n",
    "    return resized_observation\n",
    "\n",
    "#slight change of notations\n",
    "#ok let next_state be called x_t1 and current state be x_t\n",
    "# let reward be r_t and action a_t, 9=env.action_space.n= possible actions\n",
    "x_t = env.reset()\n",
    "x_t = preprocess_observation(x_t)\n",
    "print (\"x_t.shape = %s\"% (x_t.shape,))\n",
    "\n",
    "s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "print (\"s_t.shape = %s\"% (s_t.shape,))\n",
    "s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*84*84*4\n",
    "print (\"s_t.shape now = %s\"% (s_t.shape,))\n",
    "\n",
    "#above stuff - one time per episode at the start eventualy set s_t = s_t1 and so on \n",
    "\n",
    "a_t = random.randrange(env.action_space.n)\n",
    "x_t1 , r_t, done, info = env.step(a_t)\n",
    "print (x_t1.shape)\n",
    "\n",
    "x_t1=preprocess_observation(x_t1)\n",
    "print (\"x_t1.shape = %s\"% (x_t1.shape,))\n",
    "x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1],1) #1x84x84x1\n",
    "print (\"x_t1.shape now = %s\"% (x_t1.shape,))\n",
    "s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3)\n",
    "print (\"s_t1.shape = %s\"% (s_t1.shape,))\n",
    "#now s_t =s_t1 and then loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xsAFUvv23NSJ",
    "outputId": "4b665c3a-fcad-4f64-c677-a16fbdb81609"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([(array([[0.3745638 , 0.37254494, 0.30249557, ..., 0.30249557, 0.37254494,\n",
      "        0.3745638 ],\n",
      "       [0.53272283, 0.5218396 , 0.14421882, ..., 0.14421882, 0.5218396 ,\n",
      "        0.53272283],\n",
      "       [0.5327941 , 0.5217835 , 0.13974088, ..., 0.13974088, 0.5217835 ,\n",
      "        0.5327941 ],\n",
      "       ...,\n",
      "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
      "        0.        ]], dtype=float32), 3, 0.0, array([[[[0.3745638 ],\n",
      "         [0.37254494],\n",
      "         [0.30249557],\n",
      "         ...,\n",
      "         [0.30249557],\n",
      "         [0.37254494],\n",
      "         [0.3745638 ]],\n",
      "\n",
      "        [[0.53272283],\n",
      "         [0.5218396 ],\n",
      "         [0.14421882],\n",
      "         ...,\n",
      "         [0.14421882],\n",
      "         [0.5218396 ],\n",
      "         [0.53272283]],\n",
      "\n",
      "        [[0.5327941 ],\n",
      "         [0.5217835 ],\n",
      "         [0.13974088],\n",
      "         ...,\n",
      "         [0.13974088],\n",
      "         [0.5217835 ],\n",
      "         [0.5327941 ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.        ],\n",
      "         [0.        ],\n",
      "         [0.        ],\n",
      "         ...,\n",
      "         [0.        ],\n",
      "         [0.        ],\n",
      "         [0.        ]],\n",
      "\n",
      "        [[0.        ],\n",
      "         [0.        ],\n",
      "         [0.        ],\n",
      "         ...,\n",
      "         [0.        ],\n",
      "         [0.        ],\n",
      "         [0.        ]],\n",
      "\n",
      "        [[0.        ],\n",
      "         [0.        ],\n",
      "         [0.        ],\n",
      "         ...,\n",
      "         [0.        ],\n",
      "         [0.        ],\n",
      "         [0.        ]]]], dtype=float32), False), (array([[[[0.3745638 , 0.3745638 , 0.3745638 , 0.3745638 ],\n",
      "         [0.37254494, 0.37254494, 0.37254494, 0.37254494],\n",
      "         [0.30249557, 0.30249557, 0.30249557, 0.30249557],\n",
      "         ...,\n",
      "         [0.30249557, 0.30249557, 0.30249557, 0.30249557],\n",
      "         [0.37254494, 0.37254494, 0.37254494, 0.37254494],\n",
      "         [0.3745638 , 0.3745638 , 0.3745638 , 0.3745638 ]],\n",
      "\n",
      "        [[0.53272283, 0.53272283, 0.53272283, 0.53272283],\n",
      "         [0.5218396 , 0.5218396 , 0.5218396 , 0.5218396 ],\n",
      "         [0.14421882, 0.14421882, 0.14421882, 0.14421882],\n",
      "         ...,\n",
      "         [0.14421882, 0.14421882, 0.14421882, 0.14421882],\n",
      "         [0.5218396 , 0.5218396 , 0.5218396 , 0.5218396 ],\n",
      "         [0.53272283, 0.53272283, 0.53272283, 0.53272283]],\n",
      "\n",
      "        [[0.5327941 , 0.5327941 , 0.5327941 , 0.5327941 ],\n",
      "         [0.5217835 , 0.5217835 , 0.5217835 , 0.5217835 ],\n",
      "         [0.13974088, 0.13974088, 0.13974088, 0.13974088],\n",
      "         ...,\n",
      "         [0.13974088, 0.13974088, 0.13974088, 0.13974088],\n",
      "         [0.5217835 , 0.5217835 , 0.5217835 , 0.5217835 ],\n",
      "         [0.5327941 , 0.5327941 , 0.5327941 , 0.5327941 ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.        , 0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , 0.        ],\n",
      "         ...,\n",
      "         [0.        , 0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , 0.        ]],\n",
      "\n",
      "        [[0.        , 0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , 0.        ],\n",
      "         ...,\n",
      "         [0.        , 0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , 0.        ]],\n",
      "\n",
      "        [[0.        , 0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , 0.        ],\n",
      "         ...,\n",
      "         [0.        , 0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , 0.        ]]]],\n",
      "      dtype=float32), 3, 0.0, array([[[[0.3745638 , 0.3745638 , 0.3745638 , 0.3745638 ],\n",
      "         [0.37254494, 0.37254494, 0.37254494, 0.37254494],\n",
      "         [0.30249557, 0.30249557, 0.30249557, 0.30249557],\n",
      "         ...,\n",
      "         [0.30249557, 0.30249557, 0.30249557, 0.30249557],\n",
      "         [0.37254494, 0.37254494, 0.37254494, 0.37254494],\n",
      "         [0.3745638 , 0.3745638 , 0.3745638 , 0.3745638 ]],\n",
      "\n",
      "        [[0.53272283, 0.53272283, 0.53272283, 0.53272283],\n",
      "         [0.5218396 , 0.5218396 , 0.5218396 , 0.5218396 ],\n",
      "         [0.14421882, 0.14421882, 0.14421882, 0.14421882],\n",
      "         ...,\n",
      "         [0.14421882, 0.14421882, 0.14421882, 0.14421882],\n",
      "         [0.5218396 , 0.5218396 , 0.5218396 , 0.5218396 ],\n",
      "         [0.53272283, 0.53272283, 0.53272283, 0.53272283]],\n",
      "\n",
      "        [[0.5327941 , 0.5327941 , 0.5327941 , 0.5327941 ],\n",
      "         [0.5217835 , 0.5217835 , 0.5217835 , 0.5217835 ],\n",
      "         [0.13974088, 0.13974088, 0.13974088, 0.13974088],\n",
      "         ...,\n",
      "         [0.13974088, 0.13974088, 0.13974088, 0.13974088],\n",
      "         [0.5217835 , 0.5217835 , 0.5217835 , 0.5217835 ],\n",
      "         [0.5327941 , 0.5327941 , 0.5327941 , 0.5327941 ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.        , 0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , 0.        ],\n",
      "         ...,\n",
      "         [0.        , 0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , 0.        ]],\n",
      "\n",
      "        [[0.        , 0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , 0.        ],\n",
      "         ...,\n",
      "         [0.        , 0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , 0.        ]],\n",
      "\n",
      "        [[0.        , 0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , 0.        ],\n",
      "         ...,\n",
      "         [0.        , 0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , 0.        ],\n",
      "         [0.        , 0.        , 0.        , 0.        ]]]],\n",
      "      dtype=float32), False)], maxlen=5)\n"
     ]
    }
   ],
   "source": [
    "#visualising deque\n",
    "D = deque(maxlen=5)\n",
    "D.append((x_t, a_t, r_t, x_t1, done))\n",
    "D.append((s_t, a_t, r_t, s_t1, done))\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BkhNKt2W30Xg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wSR5ISx43P9-"
   },
   "outputs": [],
   "source": [
    "#lets begin\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, epsilon , experience_replay_capacity , minibatch_size , learning_rate ,action_size, img_dim , explore,loss_func):\n",
    "        \n",
    "        self.loss_func=loss_func\n",
    "        self.action_size=action_size\n",
    "        self.discount_factor=0.90\n",
    "        self.learning_rate=learning_rate\n",
    "        self.epsilon=epsilon\n",
    "        self.epsilon_min=0.05\n",
    "        self.batch_size=minibatch_size\n",
    "        self.train_start=1000\n",
    "        self.explore=explore\n",
    "        self.img_channels=4 #phi_length  #coz we feed in 4 stacked b&w imgs instead of 1 rbg img\n",
    "        self.processed_image_dim=img_dim\n",
    "        \n",
    "         # create replay memory using deque\n",
    "        self.D=deque(maxlen=experience_replay_capacity)\n",
    "        # create main model and target model\n",
    "        self.model=self.build_model()\n",
    "        self.target_model=self.build_model()\n",
    "\n",
    "        # initialize target model\n",
    "        self.update_target_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        \n",
    "        input_shape=(PHI_LENGTH,PREPROCESS_IMAGE_DIM,PREPROCESS_IMAGE_DIM)\n",
    "        frame=Input(input_shape)\n",
    "        #frame is a tensor object\n",
    "        c1=Convolution2D(32,kernel_size=(8,8),strides=4,activation=\"relu\",padding=\"same\")(frame)\n",
    "        c2=Convolution2D(64,kernel_size=(4,4),strides=2,activation=\"relu\",padding=\"same\")(c1)\n",
    "        c3=Convolution2D(64,kernel_size=(3,3),strides=1,activation=\"relu\",padding=\"same\")(c2)\n",
    "        \n",
    "        dense_layer=Flatten()(c3)\n",
    "        \n",
    "        dense_layer=Dense(512,activation=\"relu\")(dense_layer)\n",
    "        final=Dense(ACTION_SIZE,activation=\"linear\")(dense_layer)\n",
    "        \n",
    "        model=Model(inputs=frame,outputs=final)\n",
    "        model.compile(loss=self.loss_func,optimizer=Adam(lr=self.learning_rate))\n",
    "        \n",
    "        print(\"finish building the model\")\n",
    "        print(model.summary())\n",
    "        \n",
    "        return model\n",
    "    # after some time interval update the target model to be same with model ###Q-learning specific function\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        #copy weights from the original model to target model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "    def append_experience_replay_example(self,s_t,a_t,r_t,s_t1,done):\n",
    "        \"\"\"\n",
    "        Add an experience replay example to our agent's replay memory. If\n",
    "        memory is full, overwrite previous examples, starting with the oldest\n",
    "        \"\"\"\n",
    "        #D is a memory cell\n",
    "        #Records State,Action,Reward,Next State and the boolean done\n",
    "        D.append((s_t, a_t, r_t, s_t1, done))\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= (self.epsilon - self.epsilon_min) /self.explore\n",
    "            #this is just one way of reducing exploration, try out other ways, experiment\n",
    "        \n",
    "    def preprocess_observation(self, observation, prediction=False):\n",
    "        \"\"\"\n",
    "        Helper function for preprocessing an observation for consumption by our\n",
    "        deep learning network\n",
    "        \"\"\"\n",
    "        grayscale_observation = color.rgb2gray(observation)\n",
    "        resized_observation = transform.resize(grayscale_observation, (self.processed_image_dim, self.processed_image_dim)).astype('float32')\n",
    "        if prediction:\n",
    "            resized_observation = np.expand_dims(resized_observation,0)\n",
    "        return resized_observation\n",
    "        \n",
    "    def take_action(self, s_t):\n",
    "        \"\"\"\n",
    "        Given an observation, the model attempts to take an action\n",
    "        according to its q-function approximation\n",
    "        \"\"\"\n",
    "        #We take an action based on our current epsilon value\n",
    "        #This is called Epsilon greedy exploration/exploitation\n",
    "        \n",
    "        if np.random.rand()<=self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        actions=self.model.predict(s_t)\n",
    "        return np.argmax(actions[0])\n",
    "        \n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Allow the model to collect examples from its experience replay memory\n",
    "        and learn from them\n",
    "        \"\"\"\n",
    "        minibatch=random.sample(self.D,MINIBATCH_SIZE)\n",
    "        for s_t,a_t,r_t,s_t1,done in minibatch:\n",
    "            target=self.model.predict(s_t)\n",
    "            if done:\n",
    "                target[0][a_t]=r_t\n",
    "            else:\n",
    "                # a = self.model.predict(next_state)[0]\n",
    "                t=self.target_model.predict(s_t1)[0]\n",
    "                target[0][a_t]=r_t+self.discount_factor*np.amax(t)\n",
    "                # target[0][action] = reward + self.gamma * t[np.argmax(a)]\n",
    "            self.model.fit(s_t, target, epochs=1, verbose=0)\n",
    "            \n",
    "    def load_model(self,name):\n",
    "        self.model.load_weights(name)\n",
    "    \n",
    "    def save_model(self,name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8LRbxAoQ3USG"
   },
   "outputs": [],
   "source": [
    "def run_simulation():\n",
    "    \"\"\"\n",
    "    Entry-point for running env simulation\n",
    "    \"\"\"\n",
    "\n",
    "    #print game parameters\n",
    "    print (\"~~~Environment Parameters~~~\")\n",
    "    print (\"Num episodes: %s\" % NUM_EPISODES)\n",
    "    print (\"Action space: %s\" % env.action_space)\n",
    "    print()\n",
    "    print (\"~~~Agent Parameters~~~\")\n",
    "    print (\"Epsilon: %s\" % EPSILON)\n",
    "    print (\"Experience Replay Capacity: %s\" % EXPERIENCE_REPLAY_CAPACITY)\n",
    "    print (\"Minibatch Size: %s\" % MINIBATCH_SIZE)\n",
    "    print (\"Learning Rate: %s\" % LEARNING_RATE)\n",
    "\n",
    "    #initialize agent\n",
    "    agent = Agent(epsilon=EPSILON,experience_replay_capacity=EXPERIENCE_REPLAY_CAPACITY,minibatch_size=MINIBATCH_SIZE,learning_rate=LEARNING_RATE, action_size =ACTION_SIZE, img_dim =PREPROCESS_IMAGE_DIM ,explore =EXPLORE,loss_func=ERROR_FUNCTION)\n",
    "    \n",
    "    scores,episodes=[],[] \n",
    "    #initialize auxiliary data structures\n",
    "    state_list=[] \n",
    "    tot_frames=0\n",
    "\n",
    "    for i_episode in range(NUM_EPISODES):\n",
    "        print (\"Episode: %s\" % i_episode)\n",
    "         \n",
    "        done = False\n",
    "        score = 0\n",
    "        x_t = env.reset()\n",
    "        x_t = agent.preprocess_observation(x_t)   \n",
    "        s_t = np.stack((x_t, x_t, x_t, x_t), axis=2) #how many consecutive frames to stack depends on your PHI\n",
    "        s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  \n",
    "        \n",
    "        while not done:\n",
    "            #env.render()\n",
    "            #get action for the current state and go one step in environment\n",
    "            #get action, change score and learn from memory\n",
    "            \n",
    "            a_t=agent.take_action(s_t)\n",
    "            s_t1,r_t,done,_=env.step(a_t)\n",
    "            score+=r_t if not done else -10\n",
    "            agent.append_experience_replay_example(s_t,a_t,r_t,s_t1,done)\n",
    "            s_t=s_t1\n",
    "            \n",
    "        if done:\n",
    "            # every episode update the target model to be same with model\n",
    "            agent.update_target_model() \n",
    "            scores.append(score)\n",
    "            episodes.append(i_episode)\n",
    "            print( \"  score:\", score, \"  epsilon:\", agent.epsilon)\n",
    "            \n",
    "        while True:\n",
    "            #ensure state list is populated\n",
    "            if tot_frames < PHI_LENGTH:\n",
    "                state_list.append(preprocess_observation(x_t))\n",
    "                tot_frames += 1\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "                #update state list with next observation\n",
    "                state_list.append(preprocess_observation(x_t))\n",
    "                state_list.pop(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "islwg7ES3WQD",
    "outputId": "c012f9ee-b646-4382-a9ee-3fdc5c735270"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~Environment Parameters~~~\n",
      "Num episodes: 100\n",
      "Action space: Discrete(9)\n",
      "\n",
      "~~~Agent Parameters~~~\n",
      "Epsilon: 1\n",
      "Experience Replay Capacity: 2000\n",
      "Minibatch Size: 100\n",
      "Learning Rate: 0.01\n",
      "finish building the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 4, 84, 84)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 1, 21, 32)         172064    \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 1, 11, 64)         32832     \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 1, 11, 64)         36928     \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 704)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 512)               360960    \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 9)                 4617      \n",
      "=================================================================\n",
      "Total params: 607,401\n",
      "Trainable params: 607,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "finish building the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        (None, 4, 84, 84)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 1, 21, 32)         172064    \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 1, 11, 64)         32832     \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 1, 11, 64)         36928     \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 704)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 512)               360960    \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 9)                 4617      \n",
      "=================================================================\n",
      "Total params: 607,401\n",
      "Trainable params: 607,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Episode: 0\n",
      "  score: 150.0   epsilon: 0.999833764518323\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-4519ca11f186>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MsPacman-v0\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#env name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrun_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-a4d810c71abc>\u001b[0m in \u001b[0;36mrun_simulation\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;31m#ensure state list is populated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mtot_frames\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mPHI_LENGTH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m                 \u001b[0mstate_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mtot_frames\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env =gym.make(\"MsPacman-v0\") #env name\n",
    "    run_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qzjEGr-r3Y-A"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Deep Q Network-Pacman-Vanilla.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
